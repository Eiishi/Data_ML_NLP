{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44c382de",
   "metadata": {},
   "source": [
    "# Classification non-supervisée de questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beaa521",
   "metadata": {},
   "source": [
    "## Import des librairies et des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9c07f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffc586ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c56bfb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "stop_words = nltk.corpus.stopwords.words(\"english\")\n",
    "for word in ['what', 'how', 'where', 'who', 'which'] :\n",
    "    stop_words.append(word)\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "184f48cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a7740bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'smart_open' has no attribute 'local_file' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpora\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcorpora\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m simple_preprocess\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/gensim/__init__.py:11\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4.1.2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m     14\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgensim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mhandlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/gensim/parsing/__init__.py:4\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"This package contains functions to preprocess raw text\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mporter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PorterStemmer  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     preprocess_documents,\n\u001b[1;32m      6\u001b[0m     preprocess_string,\n\u001b[1;32m      7\u001b[0m     read_file,\n\u001b[1;32m      8\u001b[0m     read_files,\n\u001b[1;32m      9\u001b[0m     remove_stopwords,\n\u001b[1;32m     10\u001b[0m     split_alphanum,\n\u001b[1;32m     11\u001b[0m     stem_text,\n\u001b[1;32m     12\u001b[0m     strip_multiple_whitespaces,\n\u001b[1;32m     13\u001b[0m     strip_non_alphanum,\n\u001b[1;32m     14\u001b[0m     strip_numeric,\n\u001b[1;32m     15\u001b[0m     strip_punctuation,\n\u001b[1;32m     16\u001b[0m     strip_short,\n\u001b[1;32m     17\u001b[0m     strip_tags,\n\u001b[1;32m     18\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/gensim/parsing/preprocessing.py:26\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstring\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparsing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mporter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PorterStemmer\n\u001b[1;32m     30\u001b[0m STOPWORDS \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfrozenset\u001b[39m([\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msix\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjust\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mless\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeing\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindeed\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mover\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmove\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124manyway\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfour\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mown\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthrough\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124musing\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfifty\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhere\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmill\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124monly\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfind\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mone\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhose\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhow\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msomewhere\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmake\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124monce\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     59\u001b[0m ])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/gensim/utils.py:36\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msmart_open\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28mopen\u001b[39m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m gensim_version\n\u001b[1;32m     40\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/smart_open/__init__.py:34\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m logger\u001b[38;5;241m.\u001b[39maddHandler(logging\u001b[38;5;241m.\u001b[39mNullHandler())\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msmart_open\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msmart_open_lib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28mopen\u001b[39m, parse_uri, smart_open, register_compressor  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     36\u001b[0m _WARNING \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124msmart_open.s3_iter_bucket is deprecated and will stop functioning\u001b[39m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124min a future version. Please import iter_bucket from the smart_open.s3 module instead:\u001b[39m\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m \u001b[38;5;124m    from smart_open.s3 import iter_bucket as s3_iter_bucket\u001b[39m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     42\u001b[0m _WARNED \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/smart_open/smart_open_lib.py:35\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msmart_open\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlocal_file\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mso_file\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msmart_open\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompression\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mso_compression\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msmart_open\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m doctools\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msmart_open\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transport\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# For backwards compatibility and keeping old unit tests happy.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/smart_open/doctools.py:21\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compression\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transport\n\u001b[1;32m     23\u001b[0m PLACEHOLDER \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m    smart_open/doctools.py magic goes here\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_kwargs\u001b[39m(docstring):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/smart_open/transport.py:22\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     20\u001b[0m NO_SCHEME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 22\u001b[0m _REGISTRY \u001b[38;5;241m=\u001b[39m {NO_SCHEME: \u001b[43msmart_open\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_file\u001b[49m}\n\u001b[1;32m     23\u001b[0m _ERRORS \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     24\u001b[0m _MISSING_DEPS_ERROR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mYou are trying to use the \u001b[39m\u001b[38;5;132;01m%(module)s\u001b[39;00m\u001b[38;5;124m functionality of smart_open\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124mbut you do not have the correct \u001b[39m\u001b[38;5;132;01m%(module)s\u001b[39;00m\u001b[38;5;124m dependencies installed. Try:\u001b[39m\n\u001b[1;32m     26\u001b[0m \n\u001b[1;32m     27\u001b[0m \u001b[38;5;124m    pip install smart_open[\u001b[39m\u001b[38;5;132;01m%(module)s\u001b[39;00m\u001b[38;5;124m]\u001b[39m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'smart_open' has no attribute 'local_file' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ccde77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7360058a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d9fef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandarallel import pandarallel\n",
    "\n",
    "pandarallel.initialize(progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4a1b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6431a8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"top_10_tags.txt\", \"r\")\n",
    "top_10_tags = file.read()\n",
    "top_10_tags = list(top_10_tags.split('\\n')[:-1])\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982d9e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12b8ae4",
   "metadata": {},
   "source": [
    "## Échantillonnage et nettoyage des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92df0bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = data['Title']\n",
    "text_spl = text.sample(frac = 0.25).reset_index(drop = True)\n",
    "text_spl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eb51c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Textes bruts :\")\n",
    "print(\"\")\n",
    "print(text_spl[:11])\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Textes nettoyés par Gensim :\")\n",
    "print(\"\")\n",
    "print(text_spl[:11].apply(simple_preprocess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdd8e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags = [\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"]) :\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable = [\"parser\", \"ner\"])\n",
    "    texts_out = []\n",
    "    for text in texts :\n",
    "        doc = nlp(text)\n",
    "        new_text = []\n",
    "        for token in doc :\n",
    "            if token.orth_ in top_10_tags :\n",
    "                new_text.append(token.orth_)\n",
    "            else :\n",
    "                if token.pos_ in allowed_postags :\n",
    "                    new_text.append(token.lemma_)\n",
    "        final = \" \".join(new_text)\n",
    "        texts_out.append(final)\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb510f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Textes bruts :\")\n",
    "print(\"\")\n",
    "print(text_spl[:11])\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Textes nettoyés par spaCy :\")\n",
    "print(\"\")\n",
    "print(pd.Series(lemmatization(text_spl[:11])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7123a39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text) :\n",
    "\n",
    "    \"\"\"\" Nettoyage du texte :\n",
    "    passage au minuscule\n",
    "    suppression du code éventuel du texte que l'on stocke dans une variable 'code'\n",
    "    suppression et du contenu des balises autres que p (script, alt, ...)\n",
    "    suppression des balises html\n",
    "    conservation des textes labellisés par les top 10 tags uniquement\n",
    "    suppression de la ponctuation, des chiffres,\n",
    "    et des stopwords\n",
    "    lemmatisation par spaCy \"\"\"\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    soup = BeautifulSoup(text)\n",
    "    \n",
    "    if soup.find(\"code\") :        \n",
    "        code = soup.find(\"code\").get_text()\n",
    "        soup.find('code').clear()\n",
    "    text_wo_tags = soup.get_text()\n",
    "    \n",
    "    for i in range(1, len(text_wo_tags)) :\n",
    "        if text_wo_tags[i-1] == 'c' and text_wo_tags[i] == '#' :\n",
    "            text_wo_tags = text_wo_tags.replace(text_wo_tags[i], 'sharp')\n",
    "    \n",
    "    token_list = nltk.word_tokenize(text_wo_tags)\n",
    "    \n",
    "    new_text = []\n",
    "    \n",
    "    for token in token_list :\n",
    "        if token in top_10_tags :\n",
    "            new_text.append(token)\n",
    "        elif token not in stop_words :\n",
    "            for char in token :\n",
    "                if char in punctuation or char.isdigit() :\n",
    "                    token = token.replace(char, '')\n",
    "            new_text.append(token)\n",
    "    \n",
    "    lem = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    for token in new_text :\n",
    "        if nltk.pos_tag([token])[0][1].startswith('V') :\n",
    "            index = new_text.index(token)\n",
    "            token_lem = lem.lemmatize(token, pos = 'v')\n",
    "            new_text[index] = new_text[index].replace(token, token_lem)\n",
    "            \n",
    "    new_text = ' '.join(new_text)\n",
    "\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dee0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Textes bruts :\")\n",
    "print(\"\")\n",
    "print(text_spl[:11])\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Textes nettoyés par la fonction créée :\")\n",
    "print(\"\")\n",
    "print(text_spl[:11].apply(preprocess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98df54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "text_clean = text_spl.parallel_apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4477808b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_spl = pd.DataFrame(text_spl)\n",
    "text_clean = pd.DataFrame(text_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783bb655",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data, text_spl, on = \"Title\", how = \"right\")\n",
    "data = data.drop(columns = {'Body'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0e9268",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data, text_clean], axis = 1)\n",
    "data.columns = ['Title', 'Tags', 'Title_clean'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffaed08",
   "metadata": {},
   "source": [
    "## Classification non-supervisée"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560ddc4e",
   "metadata": {},
   "source": [
    "### Feature extraction par Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89197d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for doc in text_clean :\n",
    "    words.append(nltk.word_tokenize(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5181f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(words)\n",
    "corpus = []\n",
    "\n",
    "for word in words :\n",
    "    corpus.append(id2word.doc2bow(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0a7b31",
   "metadata": {},
   "source": [
    "### Optimisation du nombre de topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d8c0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coherence_table(corpus, dictionary, list_n):\n",
    "    \n",
    "    coherence_table = []\n",
    "    \n",
    "    for i in list_n :\n",
    "    \n",
    "        lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                               id2word=id2word,\n",
    "                                               num_topics=i, \n",
    "                                               random_state=100,\n",
    "                                               chunksize=100,\n",
    "                                               passes=10)\n",
    "\n",
    "        coherence_model_lda = CoherenceModel(model=lda_model, texts=words, dictionary=id2word, coherence='c_v')\n",
    "\n",
    "        coherence_table.append(coherence_model_lda.get_coherence())\n",
    "\n",
    "    return coherence_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9469f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_topics_range = np.linspace(3, 30, 10)\n",
    "table = coherence_table(corpus, id2word, n_topics_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f182ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(y = table, x = n_topics_range).set(xlabel = \"n_topics\", ylabel = \"Cohérence\")\n",
    "plt.title(\"Score de cohérence du modèle de LDA en fonction du nombre de topics\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a39f98",
   "metadata": {},
   "source": [
    "### Clustering par Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40365ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 15\n",
    "\n",
    "lda = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=num_topics, \n",
    "                                           random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff7e435",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "gensimvis.prepare(lda, corpus, id2word, mds = 'mmds', R=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9563a593",
   "metadata": {},
   "source": [
    "### Extraction des tags trouvés par LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b9e965",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_df = []\n",
    "\n",
    "for row in lda.show_topics(num_topics = 15) :\n",
    "    for tag in top_10_tags :\n",
    "        if tag in row[1] :\n",
    "            lda_df.append([row[0], tag])\n",
    "            \n",
    "lda_df = pd.DataFrame(lda_df, columns = ['num_cluster', 'tag_lda'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be4c13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_df = pd.DataFrame(lda_df.groupby('num_cluster')['tag_lda'].apply(list)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a9883e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9c7711",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_cluster = []\n",
    "\n",
    "for index, row in enumerate(lda[corpus]) :\n",
    "    if len(row) < 15 :\n",
    "        lda_cluster.append([index, row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25053b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_cluster = pd.DataFrame(lda_cluster, columns = ['data_index', 'lda_cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fcd2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_cluster = pd.DataFrame(lda_cluster.explode('lda_cluster').explode('lda_cluster')[::2].groupby('data_index')['lda_cluster'].apply(\n",
    "    list)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259e4257",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[lda_cluster['data_index'].values.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abdb4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data, lda_cluster, left_index = True, right_on = \"data_index\").drop(columns = 'data_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33555ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_topics(data) :\n",
    "\n",
    "    lda_tags = []\n",
    "\n",
    "    for cluster in range(len(lda_df['num_cluster'])) :\n",
    "        if cluster in data :\n",
    "            tmp = []\n",
    "            tmp.append(lda_df.loc[cluster, 'tag_lda'])\n",
    "            tmp = pd.Series(tmp).explode().to_list()\n",
    "            lda_tags = lda_tags + tmp\n",
    "            \n",
    "    return lda_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f3ff31",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lda_tags'] = data['lda_cluster'].apply(find_topics)\n",
    "data = data.drop(columns = {'lda_cluster'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912d7b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
