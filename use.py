# -*- coding: utf-8 -*-
"""USE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Oa55w0pUJG5ybo5CeTYl1rzxkiOt91gX
"""

import pandas as pd
import numpy as np
import time

import nltk
# nltk.download('stopwords')
# nltk.download('punkt')
# nltk.download('averaged_perceptron_tagger')
# nltk.download('wordnet')
stop_words = nltk.corpus.stopwords.words("english")
for word in ['what', 'how', 'where', 'who', 'which'] :
    stop_words.append(word)
from string import punctuation

import spacy

import tensorflow_hub as hub

from sklearn.model_selection import train_test_split
from sklearn.metrics import jaccard_score
from sklearn.linear_model import LogisticRegression
from sklearn.multioutput import MultiOutputClassifier

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import FunctionTransformer

from joblib import dump



file = open("./top_10_tags.txt", "r")
top_10_tags = file.read()
top_10_tags = list(top_10_tags.split('\n')[:-1])
file.close()

data = pd.read_csv("./data.csv")



text = data['Title']
text_spl = text.sample(frac = 0.25).reset_index()
text_spl.head()

def preprocess(text) :

    """" Nettoyage du texte :
    passage au minuscule
    suppression du code éventuel du texte que l'on stocke dans une variable 'code'
    suppression et du contenu des balises autres que p (script, alt, ...)
    suppression des balises html
    conservation des textes labellisés par les top 10 tags uniquement
    suppression de la ponctuation, des chiffres,
    et des stopwords
    lemmatisation par spaCy """
    
    text = text.lower()
    
    for i in range(1, len(text)) :
        if text[i-1] == 'c' and text[i] == '#' :
            text = text.replace(text[i], 'sharp')
    
    token_list = nltk.word_tokenize(text)
    
    new_text = []
    
    for token in token_list :
        if token in top_10_tags :
            new_text.append(token)
        elif token not in stop_words :
            for char in token :
                if char in punctuation or char.isdigit() :
                    token = token.replace(char, '')
            new_text.append(token)
    
    lem = nltk.stem.WordNetLemmatizer()
    
    for token in new_text :
        if nltk.pos_tag([token])[0][1].startswith('V') :
            index = new_text.index(token)
            token_lem = lem.lemmatize(token, pos = 'v')
            new_text[index] = new_text[index].replace(token, token_lem)
            
    new_text = ' '.join(new_text)

    return new_text

# print("Textes bruts :")
# print("")
# print(text_spl.loc[:11, 'Title'])
# print("---------------------------------------")
# print("Textes nettoyés :")
# print("")
# print(text_spl.loc[:11, 'Title'].apply(preprocess))

text_clean = text_spl['Title'].apply(preprocess)

text_spl['Title_clean'] = text_clean

data = pd.merge(data.iloc[text_spl['index']], text_spl)[['Title', 'Title_clean', 'Tags']]
data.head(3)



embed = hub.load("https://tfhub.dev/google/universal-sentence-encoder/4")

def feature_USE_fct(sentences, b_size) :
    batch_size = b_size
    time1 = time.time()

    for step in range(len(sentences)//batch_size) :
        idx = step*batch_size
        feat = embed(sentences[idx:idx+batch_size])

        if step ==0 :
            features = feat
        else :
            features = np.concatenate((features,feat))

    time2 = np.round(time.time() - time1,0)
    return features

batch_size = 10
sentences = data['Title_clean'].to_list()

features_USE = feature_USE_fct(sentences, batch_size)

data = data.join(pd.DataFrame(features_USE))



for tag in top_10_tags :
    data['is' + tag] = 0
    index = 0
    for doc_tag in data['Tags'] :
        if not pd.isnull(doc_tag) :
          if tag in doc_tag :
            data.loc[index, 'is' + tag] = 1
        index += 1



X = data.iloc[:,3:-10]
y = data.iloc[:,-10:]

X = X[~X[0].isna()]
y = y.iloc[X.index]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42)

mclr = MultiOutputClassifier(LogisticRegression(max_iter = 1000)).fit(X_train, y_train)



transformer = FunctionTransformer(feature_USE_fct, kw_args={'b_size':10})

transformer.fit(X_train, y_train)
mclr.fit(X_train, y_train)

pipe = Pipeline([('USE', transformer), ('MultiLogReg', mclr)])

dump(pipe, 'trained_use_logreg.joblib')